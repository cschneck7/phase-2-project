{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Home Improvements\n",
    "![Hero Lake Washington, King County](images/hero-lake-washington-xlg.jpg)\n",
    "<br>\n",
    "\n",
    "**Author**: Carl Schneck <br>\n",
    "**Program**: Data Science Flex <br>\n",
    "**Phase 2 Project**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import stats\n",
    "import data_preparation_functions as dp\n",
    "import figure_functions as fg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project analyzes King County housing sales data in order to help lead a wholesale real estate investor make educated decisions on which home improvements best improve sales prices in the area. King County is the most populous county in Washington State, and 13th in the country.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "A wholesale real estate investor wants to get a better idea of which improvements relate to the the biggest increase in sales price. By finding out this information they can better access which projects are more worthwile and lead to the largest profit margin. Through a linear regression analysis we can figure out which features have the largest affect on sales price by looking at our models coefficients. \n",
    "\n",
    "The area of interest is King County, Washington. Which is the most populous county of Washington state, and ranked 13th in the country. In order to complete the objective a dataset containing sales data for King County spanning the years 2014 to 2015 was analyzed. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Our dataset includes information on houses sold in the timeframe spanning the years 2014 through 2015 and 70 zipcodes of King County. There are a total of 21,597 entries with 21 columns worth of information. For this analysis we will cut down this data to features we believe will be helpful for the client. These include features that can be improved after the purchase of a home. Features dealing with location, view or neighbooring properties are things that in most cases are impossible to change so will be ommited.    \n",
    "\n",
    "After taking the above into consideration we began modelling with the features below, with `price` being our target variable. For a more in depth analysis of why the other features were not included, as well as the the preperation and modeling process, please observe the EDA and Modeling notebook linked <a href=\"EDA_and_Modelling.ipynb\">here</a>.\n",
    "\n",
    "- `price` - Sales price of house\n",
    "- `bedrooms` - Number of bedrooms\n",
    "- `bathrooms` - Number of bathrooms\n",
    "- `sqft_living` - Square footage of living space\n",
    "- `floors` - Number of floors\n",
    "- `condition` - Overall maintanence condition of the house\n",
    "    - 1 = Poor\n",
    "    - 2 = Fair\n",
    "    - 3 = Average\n",
    "    - 4 = Good\n",
    "    - 5 = Very Good\n",
    "- `grade` - Overall construction and design grade of the house\n",
    "    - 3 = Poor\n",
    "    - 4 = Low\n",
    "    - 5 = Fair\n",
    "    - 6 = Low Average\n",
    "    - 7 = Average\n",
    "    - 8 = Good\n",
    "    - 9 = Better\n",
    "    - 10 = Very Good\n",
    "    - 11 = Excellent\n",
    "    - 12 = Luxury\n",
    "    - 13 = Mansion\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "There were a few steps taken to prepare the data. \n",
    "\n",
    "1. Drop the unnecessary columns\n",
    "2. Convert the categorical columns to numerical columns\n",
    "    - (`grade`, `condition`)\n",
    "3. Drop any duplicates\n",
    "4. Drop any outliers\n",
    "\n",
    "The initial dataframe after preparation is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>floors</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bedrooms  bathrooms  sqft_living  floors  condition  grade\n",
       "0  221900.0         3       1.00         1180     1.0          3      7\n",
       "1  538000.0         3       2.25         2570     2.0          3      7\n",
       "2  180000.0         2       1.00          770     1.0          3      6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads initial prepared dataframe from the EDA_and_Modelling notebook\n",
    "init_df = pd.read_pickle('data/init_df.pk1')\n",
    "init_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was then split into a training and test set with a ratio of 4:1 in order to have a dataset to validate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "In order to solve our clients problem we need to be able to infer information from our model, therefore the inner mechanisms have to be kept simple and understandable. One type of model that can fit this description is a Linear Regression Model. In theory once the model is complete we should be able to pull out the coefficients related to each feature, and have a good approximation on their relationships with the target variable. Thus this type of model was chosen for this project.\n",
    "\n",
    "After much trial and error the final model only contained three features.\n",
    "\n",
    "- `sqft_living`\n",
    "- `grade`\n",
    "- `condition`\n",
    "\n",
    "This was mostly due to the fact that some features had high correlation to each other causing the models to perform poorly. These included the relationships of `bedrooms` and `bathrooms` with `sqft_living`. This makes sense considering they are part of the living space and adding to the square footage. The feature `floors` was later taken out due to it's high p-value, thus providing a lack of confidence in it's results.\n",
    "\n",
    "The model also had to be limited to homes with a sales price under 1.5 million dollars. This was performed because the `price` variable had a long right tail, causing the model to lack normality of its residuals.\n",
    "\n",
    "Scaling was also performed on the model since the variable `sqft_living` contained much higher values compared to the other two features, possibly causing a bias in the results.\n",
    "\n",
    "The final model can be previewd below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads final model training sets\n",
    "X_train = pd.read_pickle('data/X_train_final.pk1')\n",
    "y_train = pd.read_pickle('data/y_train_final.pk1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.521</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.521</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   6068.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 28 Mar 2023</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:09:52</td>     <th>  Log-Likelihood:    </th> <td>-2.2574e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 16760</td>      <th>  AIC:               </th>  <td>4.515e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 16756</td>      <th>  BIC:               </th>  <td>4.515e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>-1.387e+05</td> <td> 7379.186</td> <td>  -18.795</td> <td> 0.000</td> <td>-1.53e+05</td> <td>-1.24e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th> <td> 7.439e+05</td> <td> 1.68e+04</td> <td>   44.408</td> <td> 0.000</td> <td> 7.11e+05</td> <td> 7.77e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition</th>   <td>  1.97e+05</td> <td> 8317.684</td> <td>   23.685</td> <td> 0.000</td> <td> 1.81e+05</td> <td> 2.13e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade</th>       <td>  7.75e+05</td> <td> 1.43e+04</td> <td>   54.120</td> <td> 0.000</td> <td> 7.47e+05</td> <td> 8.03e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2333.887</td> <th>  Durbin-Watson:     </th> <td>   1.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4494.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.881</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 4.826</td>  <th>  Cond. No.          </th> <td>    20.1</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.521\n",
       "Model:                            OLS   Adj. R-squared:                  0.521\n",
       "Method:                 Least Squares   F-statistic:                     6068.\n",
       "Date:                Tue, 28 Mar 2023   Prob (F-statistic):               0.00\n",
       "Time:                        02:09:52   Log-Likelihood:            -2.2574e+05\n",
       "No. Observations:               16760   AIC:                         4.515e+05\n",
       "Df Residuals:                   16756   BIC:                         4.515e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const       -1.387e+05   7379.186    -18.795      0.000   -1.53e+05   -1.24e+05\n",
       "sqft_living  7.439e+05   1.68e+04     44.408      0.000    7.11e+05    7.77e+05\n",
       "condition     1.97e+05   8317.684     23.685      0.000    1.81e+05    2.13e+05\n",
       "grade         7.75e+05   1.43e+04     54.120      0.000    7.47e+05    8.03e+05\n",
       "==============================================================================\n",
       "Omnibus:                     2333.887   Durbin-Watson:                   1.995\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4494.887\n",
       "Skew:                           0.881   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.826   Cond. No.                         20.1\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates and previews final model\n",
    "model = dp.model_summary(X_train, y_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade          774981.565432\n",
       "sqft_living    743945.080512\n",
       "condition      197005.386528\n",
       "const         -138691.105288\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top two features from this model are `sqft_living` and `grade`, thus will be the recommended features to improve to reach the greatest profit. \n",
    "\n",
    "Since the model was scaled these coefficients cannot be directly used. The independant features of the model were scaled using the formula below.\n",
    "\n",
    "<br>\n",
    "<center>$$x_{iscaled} = \\displaystyle \\frac{x_{i} - x_{min}}{x_{max} - x_{min}}$$</center>\n",
    "<br>\n",
    "\n",
    "- $x_{iscaled}$ is the scaled value of $x_{i}$\n",
    "- $x_{i}$ are the individual values in feature x\n",
    "- $x_{min}$ is the minimun value in feature x\n",
    "- $x_{max}$ is the maximum value in feature x\n",
    "\n",
    "What's important for our analysis is the rate of change. Since the transformation was linear we only need to take the denominator into consideration. Thus the rate of change per unit of the feature can be found by using the equation below, where c is the coeffiecient of the feature calculated by the model.\n",
    "\n",
    "<br>\n",
    "<center>$$rate = \\displaystyle \\frac{c}{x_{max} - x_{min}}$$</center>\n",
    "<br>\n",
    "\n",
    "The rate of change per unit has been saved in the dataseries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads rates of change per unit from pickle file\n",
    "rates = pd.read_pickle('data/rates.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Extra Figures??\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Any Works Cited\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
